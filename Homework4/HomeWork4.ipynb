{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import sentence_polarity\n",
    "import random\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import stopwords\n",
    "from Subjectivity import *\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.metrics import precision\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.metrics import ConfusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/komal/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package sentence_polarity to\n",
      "[nltk_data]     /Users/komal/nltk_data...\n",
      "[nltk_data]   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/komal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('sentence_polarity')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mycorpus = PlaintextCorpusReader('.','clothing_shoes_jewelry.txt')\n",
    "type(mycorpus.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviewerID:A1KLRMWW2FWPL4\n",
      "asin:0000031887\n",
      "reviewerName:Amazon Customer \"cameramom\"\n",
      "helpful:[0, 0]\n",
      "reviewText:This is a great tutu and at a really great price. It doesn't look cheap at all. I'm so glad I looked on Amazon and found such an affordable tutu that isn't made poorly. A++\n",
      "overall:5.0\n",
      "summary:Great tutu-  not cheaply made\n",
      "unixReviewTime:1297468800\n",
      "reviewTime:02 12, 2011\n",
      "\n",
      "reviewerID:A2G5TCU2WDFZ65\n",
      "asin:0000031887\n",
      "reviewerName:Amazon Customer\n",
      "helpful:[0, 0]\n",
      "reviewText:I bought this for my \n"
     ]
    }
   ],
   "source": [
    "file_id = mycorpus.fileids()\n",
    "file_contents = []\n",
    "file = file_id[0]\n",
    "file_handle = open(file, 'r')\n",
    "file_content = file_handle.read()\n",
    "file_handle.close()\n",
    "print(file_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all reviews\n",
    "\n",
    "pword = re.compile('reviewerID:(.*)\\nasin:(.*)\\nreviewerName:(.*)\\nhelpful:(.*)\\nreviewText:(.*)\\n(overall:.*)\\nsummary:(.*)\\nunixReviewTime:(.*)\\nreviewTime:(.*)\\n')\n",
    "reviews = re.findall(pword,file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "278225"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2014"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract the year\n",
    "int(reviews[11][8][7:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of reviews in 2014 : 67614\n"
     ]
    }
   ],
   "source": [
    "# count number of reviews in 2014\n",
    "counter = 0\n",
    "for i in range(len(reviews)):\n",
    "    year = int(reviews[i][8][7:])\n",
    "    if (year == 2014):\n",
    "        # only [i][4] corresponds to the reviewText field. \n",
    "        counter+=1\n",
    "\n",
    "print(\"number of reviews in 2014 : \" + str(counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews_2014 only first 5500 contains the reviews of year 2014\n",
    "reviews_2014 = []\n",
    "counter = 0\n",
    "for i in range(len(reviews)):\n",
    "    year = int(reviews[i][8][7:])\n",
    "    if (year == 2014):\n",
    "        # only [i][4] corresponds to the reviewText field. \n",
    "        reviews_2014.append(reviews[i][4])\n",
    "        counter+=1\n",
    "        if (counter >= 5500):\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5500"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews_2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We bought several tutus at once, and they are got high reviews. Sturdy and seemingly well-made. The girls have been wearing them regularly, including out to play, and the tutus have stood up well. Fits the 3-yr old & the 5-yr old well. Clearly plenty of room to grow. Only con is that when the kids pull off the tutus, the waste band gets twisted, and an adult has to un-tangle. But this is not difficult.',\n",
       " \"Thank you Halo Heaven great product for Little Girls.  My Great Grand Daughters Love these Tutu's.  Will buy more from this seller.  Made well and cute on the girls.  Thanks for a great product.NEVER BUY FROM DRESS UP DREAMS........I will buy more as long as I don't buy from &#34;Dress Up Dreams&#34;  I never rec'd or order in FL. Only rec'd pink, the purple one was missing.  Company is a rip off.  REFUSES to make good on purchase...... Real creeps.\"]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_2014[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence level tokenization\n",
    "sentences=[nltk.sent_tokenize(sent) for sent in reviews_2014]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"It won't hold heavy canned goods like a cotton canvas bag, and is a little smaller than paper grocery bag.\",\n",
       " 'Good for the beach or knitting carryall.',\n",
       " 'The fabric seems to be polyester with a waterproof coating inside that also stabilizes it.',\n",
       " 'The plastic smell does go away in a couple of days IF you open them up to air out.',\n",
       " \"There's no stiffener in the bottom so it folds very flat.\",\n",
       " 'While the seams are stitched adequately, they are not finished on the inside, though a couple of colors do have taped seams.',\n",
       " 'But for $3.50, they are a bargain.']"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21629\n",
      "[['I', 'took', 'it', 'to', 'ups', 'myself', 'and', 'paid', 'the', 'return', 'with', '$', '100.00', 'insurance', 'secured', 'for', 'what', 'look', 'like', 'are', 'boots', 'worth', '$', '20', '.'], [\"I'm\", 'sorry', 'I', 'have', 'not', 'had', 'any', 'issues', 'ever', 'with', 'Amazon', 'cause', 'your', 'stuff', 'is', 'great', 'and', \"I'm\", 'always', 'happy', 'with', 'my', 'purchases', 'but', 'since', 'I', 'kept', 'my', 'receipts', 'on', 'the', 'returns', 'of', 'these', 'items', 'please', 'ask', 'your', '3rd', 'party', 'seller', 'what', 'happen', '!']]\n"
     ]
    }
   ],
   "source": [
    "# tokenize each review\n",
    "# Sentence level tokenization, I am using Tweet Tokenizer because \n",
    "# if we use nltk.sent_tokenize, the words like didn't gets split, which we dont want.\n",
    "\n",
    "tknzr = TweetTokenizer()\n",
    "original_tokenized_review_sentences = []\n",
    "for each_review in sentences:\n",
    "    # tokenize each review into words by splitting them into different sentences\n",
    "    sent_wordlevel=[tknzr.tokenize(sent) for sent in each_review]\n",
    "    for each in sent_wordlevel:\n",
    "        original_tokenized_review_sentences.append(each)\n",
    "        \n",
    "print(len(original_tokenized_review_sentences))\n",
    "print(original_tokenized_review_sentences[-3:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'took', 'it', 'to', 'ups', 'myself', 'and', 'paid', 'the', 'return', 'with', '$', '100.00', 'insurance', 'secured', 'for', 'what', 'look', 'like', 'are', 'boots', 'worth', '$', '20', '.'], [\"i'm\", 'sorry', 'i', 'have', 'not', 'had', 'any', 'issues', 'ever', 'with', 'amazon', 'cause', 'your', 'stuff', 'is', 'great', 'and', \"i'm\", 'always', 'happy', 'with', 'my', 'purchases', 'but', 'since', 'i', 'kept', 'my', 'receipts', 'on', 'the', 'returns', 'of', 'these', 'items', 'please', 'ask', 'your', '3rd', 'party', 'seller', 'what', 'happen', '!']]\n"
     ]
    }
   ],
   "source": [
    "# converting the sentences to lower case to have the uniformity during classification\n",
    "tokenized_review_sentences = []\n",
    "# sentences now has the originally selected sentences from the reiviews file\n",
    "# and lower_case_sentences has the \n",
    "for sentence in original_tokenized_review_sentences:\n",
    "    tokenized_review_sentences.append([item.lower() for item in sentence])\n",
    "    \n",
    "print(tokenized_review_sentences[-3:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21629\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized_review_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'each', 'above', 'again', 'but', 'mightn', 'then', 'in', 'herself', \"it's\", 'about', 'o', \"you're\", 'did', 'shouldn', 'himself', 'him', 'the', 'both', 'myself', 'at', \"you'd\", 'now', 'couldn', 'against', 'mustn', 'they', 't', 'over', 'no', 'more', 'to', \"she's\", 'there', \"shouldn't\", 've', 'during', 'through', \"that'll\", 'y', 'do', 'll', 'i', 'his', 'hers', 'that', \"didn't\", 'with', \"wasn't\", 'any', 'didn', 'shan', 'why', 'd', 'ours', 'having', 'an', \"weren't\", 'own', 'its', 'while', 'itself', 'until', \"isn't\", 'your', 'some', 'such', 'have', 'will', 'from', 'what', 'were', 'or', 'm', 'are', 'he', 'down', \"mustn't\", 'which', \"haven't\", 'yourself', 'between', 'all', 'it', 'should', \"shan't\", 'am', \"couldn't\", 's', 'few', 'be', 'haven', 'yourselves', 'ain', 'whom', 'was', 'before', \"mightn't\", 'these', \"should've\", 'been', 'once', 'wasn', \"wouldn't\", 'aren', 'so', 'only', 'and', 'you', 'nor', 'too', 'our', 'other', \"hadn't\", 'doesn', 'here', 'ma', 'isn', 'when', 'can', 'wouldn', 'does', 'those', \"hasn't\", 'of', 'won', 'very', 'hasn', \"you've\", 'up', 'weren', 'by', 'off', 'just', 're', 'because', \"you'll\", 'she', 'not', 'my', 'how', 'ourselves', 'has', 'her', 'if', 'as', 'below', 'yours', 'being', \"doesn't\", 'don', 'me', \"don't\", 'on', \"needn't\", 'after', 'out', 'had', 'further', 'same', 'for', 'under', 'most', 'their', 'this', 'a', 'needn', 'them', 'into', 'than', 'theirs', 'where', 'we', 'hadn', 'doing', 'who', \"won't\", 'themselves', 'is', \"aren't\"}\n",
      "<class 'set'>\n"
     ]
    }
   ],
   "source": [
    "# Stop Words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)\n",
    "\n",
    "negationwords = ['no', 'not', 'never', 'none', 'nowhere', 'nothing', 'noone', 'rather', 'hardly', 'scarcely', 'rarely', 'seldom', 'neither', 'nor']\n",
    "\n",
    "neg_stop_words = []\n",
    "print(type(stop_words))\n",
    "for word in stop_words:\n",
    "    if (word in negationwords) or (word.endswith(\"n't\")):\n",
    "        neg_stop_words.append(word)\n",
    "    \n",
    "#print(neg_stop_words)\n",
    "#print(stop_words)      \n",
    "neg_stop_words = set(neg_stop_words)\n",
    "new_stop_words = []\n",
    "new_stop_words = list(stop_words - neg_stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few stop words I thought will affect the classification, \n",
    "# like too, again \n",
    "# but as we can see in the below results, it really doesnt affect \n",
    "# the classification\n",
    "\n",
    "print(new_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(swn.senti_synsets(\"too\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "breakdown3 = swn.senti_synset('besides.r.02')\n",
    "print(breakdown3.pos_score())\n",
    "print(breakdown3.neg_score())\n",
    "print(breakdown3.obj_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SentiSynset('again.r.01')]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(swn.senti_synsets(\"again\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "breakdown3 = swn.senti_synset('again.r.01')\n",
    "print(breakdown3.pos_score())\n",
    "print(breakdown3.neg_score())\n",
    "print(breakdown3.obj_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SentiSynset('very.s.01'),\n",
       " SentiSynset('identical.s.02'),\n",
       " SentiSynset('very.r.01'),\n",
       " SentiSynset('very.r.02')]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(swn.senti_synsets(\"very\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.0\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "breakdown3 = swn.senti_synset('very.s.01')\n",
    "print(breakdown3.pos_score())\n",
    "print(breakdown3.neg_score())\n",
    "print(breakdown3.obj_score())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Classification - Bag of words approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sentence_polarity.sents()\n",
    "print(sentence_polarity.categories())\n",
    "documents = [(sent, cat) for cat in sentence_polarity.categories() \n",
    "    for sent in sentence_polarity.sents(categories=cat)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [(sent, cat) for cat in sentence_polarity.categories() \n",
    "    for sent in sentence_polarity.sents(categories=cat)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_list = [word for (sent,cat) in documents for word in sent]\n",
    "all_words = nltk.FreqDist(all_words_list)\n",
    "word_items = all_words.most_common(2000)\n",
    "word_features = [word for (word, freq) in word_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(document_features(d,word_features), c) for (d,c) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.755\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = featuresets[1000:], featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print (nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos']\n",
      "0.766\n"
     ]
    }
   ],
   "source": [
    "# Trying with 3000 most frequent bag of words\n",
    "sentences = sentence_polarity.sents()\n",
    "print(sentence_polarity.categories())\n",
    "documents = [(sent, cat) for cat in sentence_polarity.categories() \n",
    "    for sent in sentence_polarity.sents(categories=cat)]\n",
    "\n",
    "random.shuffle(documents)\n",
    "\n",
    "all_words_list = [word for (sent,cat) in documents for word in sent]\n",
    "all_words = nltk.FreqDist(all_words_list)\n",
    "word_items = all_words.most_common(3000)\n",
    "word_features = [word for (word, freq) in word_items]\n",
    "\n",
    "def document_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "featuresets = [(document_features(d,word_features), c) for (d,c) in documents]\n",
    "\n",
    "train_set, test_set = featuresets[1000:], featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print (nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos']\n",
      "0.767\n"
     ]
    }
   ],
   "source": [
    "# recalculating Document Feature after removing stop words\n",
    "\n",
    "sentences = sentence_polarity.sents()\n",
    "print(sentence_polarity.categories())\n",
    "documents = [(sent, cat) for cat in sentence_polarity.categories() \n",
    "    for sent in sentence_polarity.sents(categories=cat)]\n",
    "\n",
    "random.shuffle(documents)\n",
    "\n",
    "# all_word_list after removing the stop words\n",
    "all_words_list = [word for (sent,cat) in documents for word in sent if word not in new_stop_words]\n",
    "all_words = nltk.FreqDist(all_words_list)\n",
    "word_items = all_words.most_common(3000)\n",
    "word_features = [word for (word, freq) in word_items]\n",
    "\n",
    "# bag of words approach\n",
    "def document_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "# define the feature sets using the document_features\n",
    "featuresets = [(document_features(d,word_features), c) for (d,c) in documents]\n",
    "\n",
    "# Train and test your model for accuracy\n",
    "train_set, test_set = featuresets[1000:], featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print (nltk.classify.accuracy(classifier, test_set))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Classification - Subjectivity Count features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['strongsubj', 'adj', False, 'neutral']\n",
      "2\n",
      "0\n",
      "0.73\n"
     ]
    }
   ],
   "source": [
    "SLpath = 'subjclueslen1-HLTEMNLP05.tff'\n",
    "SL = readSubjectivity(SLpath)\n",
    "print(SL['absolute'])\n",
    "\n",
    "# define the features, to find out the feature_set\n",
    "def SL_features(document, word_features, SL):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "        # count variables for the 4 classes of subjectivity\n",
    "        weakPos = 0\n",
    "        strongPos = 0\n",
    "        weakNeg = 0\n",
    "        strongNeg = 0\n",
    "        for word in document_words:\n",
    "            if word in SL:\n",
    "                strength, posTag, isStemmed, polarity = SL[word]\n",
    "                if strength == 'weaksubj' and polarity == 'positive':\n",
    "                    weakPos += 1\n",
    "                if strength == 'strongsubj' and polarity == 'positive':\n",
    "                    strongPos += 1\n",
    "                if strength == 'weaksubj' and polarity == 'negative':\n",
    "                    weakNeg += 1\n",
    "                if strength == 'strongsubj' and polarity == 'negative':\n",
    "                    strongNeg += 1\n",
    "                features['positivecount'] = weakPos + (2 * strongPos)\n",
    "                features['negativecount'] = weakNeg + (2 * strongNeg)      \n",
    "    return features\n",
    "\n",
    "\n",
    "#define the feature set for performinh the classification\n",
    "# word features here is the revised word features after removing the stop words\n",
    "SL_featuresets = [(SL_features(d, word_features, SL), c) for (d,c) in documents]\n",
    "\n",
    "print(SL_featuresets[0][0]['positivecount'])\n",
    "print(SL_featuresets[0][0]['negativecount'])\n",
    "\n",
    "train_set, test_set = SL_featuresets[1000:], SL_featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis - Negation Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "negationwords = ['no', 'not', 'never', 'none', 'nowhere', 'nothing', 'noone', 'rather', 'hardly', 'scarcely', 'rarely', 'seldom', 'neither', 'nor']\n",
    "\n",
    "def NOT_features(document, word_features, negationwords):\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = False\n",
    "        features['contains(NOT{})'.format(word)] = False\n",
    "    # go through document words in order\n",
    "    for i in range(0, len(document)):\n",
    "        word = document[i]\n",
    "        if ((i + 1) < len(document)) and ((word in negationwords) or (word.endswith(\"n't\"))):\n",
    "            i += 1\n",
    "            features['contains(NOT{})'.format(document[i])] = (document[i] in word_features)\n",
    "        else:\n",
    "            features['contains({})'.format(word)] = (word in word_features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.783\n",
      "Most Informative Features\n",
      "          contains(warm) = True              pos : neg    =     19.8 : 1.0\n",
      "    contains(engrossing) = True              pos : neg    =     19.1 : 1.0\n",
      "       contains(generic) = True              neg : pos    =     16.3 : 1.0\n",
      "       contains(routine) = True              neg : pos    =     14.9 : 1.0\n",
      "          contains(flat) = True              neg : pos    =     14.5 : 1.0\n",
      "      contains(mediocre) = True              neg : pos    =     14.3 : 1.0\n",
      "      contains(supposed) = True              neg : pos    =     14.3 : 1.0\n",
      "        contains(boring) = True              neg : pos    =     13.8 : 1.0\n",
      "     contains(inventive) = True              pos : neg    =     13.1 : 1.0\n",
      "      contains(powerful) = True              pos : neg    =     12.5 : 1.0\n",
      "   contains(mesmerizing) = True              pos : neg    =     11.7 : 1.0\n",
      "     contains(absorbing) = True              pos : neg    =     11.7 : 1.0\n",
      "    contains(refreshing) = True              pos : neg    =     11.7 : 1.0\n",
      "     contains(wonderful) = True              pos : neg    =     11.1 : 1.0\n",
      "  contains(refreshingly) = True              pos : neg    =     11.1 : 1.0\n",
      "            contains(90) = True              neg : pos    =     10.9 : 1.0\n",
      "      contains(touching) = True              pos : neg    =     10.8 : 1.0\n",
      "      contains(captures) = True              pos : neg    =     10.7 : 1.0\n",
      "        contains(stupid) = True              neg : pos    =     10.5 : 1.0\n",
      "       contains(quietly) = True              pos : neg    =     10.4 : 1.0\n",
      "          contains(ages) = True              pos : neg    =     10.4 : 1.0\n",
      "        contains(unless) = True              neg : pos    =     10.3 : 1.0\n",
      "      contains(mindless) = True              neg : pos    =     10.3 : 1.0\n",
      "          contains(dull) = True              neg : pos    =     10.0 : 1.0\n",
      "     contains(realistic) = True              pos : neg    =      9.7 : 1.0\n",
      "         contains(stale) = True              neg : pos    =      9.6 : 1.0\n",
      "      contains(provides) = True              pos : neg    =      9.4 : 1.0\n",
      "       contains(culture) = True              pos : neg    =      9.3 : 1.0\n",
      "         contains(pulls) = True              pos : neg    =      9.0 : 1.0\n",
      "        contains(tender) = True              pos : neg    =      9.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# this word_features is the list of word_features after removing the stop words\n",
    "NOT_featuresets = [(NOT_features(d, word_features, negationwords), c) for (d, c) in documents]\n",
    "NOT_featuresets[0][0]['contains(NOTlike)']\n",
    "NOT_featuresets[0][0]['contains(always)']\n",
    "\n",
    "train_set, test_set = NOT_featuresets[1000:], NOT_featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "\n",
    "classifier.show_most_informative_features(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sentence(tokenized_review_sentence):\n",
    "    sentence = \"\"\n",
    "    for i in range(len(tokenized_review_sentence)):\n",
    "        sentence+=  tokenized_review_sentence[i] + \" \"\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_file_handle = open(\"positive.txt\",\"w\")\n",
    "negative_file_handle = open(\"negative.txt\",\"w\")\n",
    "\n",
    "for i in range(len(tokenized_review_sentences)):\n",
    "    sent, orig_sent = tokenized_review_sentences[i], original_tokenized_review_sentences[i]\n",
    "    if((classifier.classify(document_features(sent,word_features))) == 'pos'):\n",
    "        positive_file_handle.write(make_sentence(orig_sent))\n",
    "        positive_file_handle.write(\"\\n\")\n",
    "    else:\n",
    "        negative_file_handle.write(make_sentence(orig_sent))\n",
    "        negative_file_handle.write(\"\\n\")\n",
    "\n",
    "positive_file_handle.close()\n",
    "negative_file_handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos']\n",
      "<class 'list'>\n",
      "<class 'dict'>\n",
      "removing tuple with key .\n",
      "removing tuple with key ,\n",
      "removing tuple with key \"\n",
      "removing tuple with key --\n",
      "removing tuple with key )\n",
      "removing tuple with key ?\n",
      "removing tuple with key (\n",
      "removing tuple with key :\n",
      "removing tuple with key '\n",
      "removing tuple with key ;\n",
      "removing tuple with key !\n",
      "removing tuple with key -\n",
      "removing tuple with key \n",
      "removing tuple with key 2\n",
      "removing tuple with key *\n",
      "removing tuple with key [a]\n",
      "removing tuple with key 90\n",
      "removing tuple with key 2002\n",
      "removing tuple with key 10\n",
      "removing tuple with key \n",
      "removing tuple with key 'the\n",
      "removing tuple with key &\n",
      "removing tuple with key 20\n",
      "removing tuple with key [the\n",
      "removing tuple with key '70s\n",
      "removing tuple with key '60s\n",
      "removing tuple with key 51\n",
      "removing tuple with key 15\n",
      "removing tuple with key /\n",
      "removing tuple with key 80\n",
      "removing tuple with key 4ever\n",
      "removing tuple with key 1\n",
      "removing tuple with key 101\n",
      "removing tuple with key 8\n",
      "removing tuple with key 30\n",
      "removing tuple with key 100\n",
      "removing tuple with key 1970s\n",
      "removing tuple with key 13\n",
      "removing tuple with key 11\n",
      "removing tuple with key 20th\n",
      "removing tuple with key 21st\n",
      "removing tuple with key 9\n",
      "removing tuple with key é\n",
      "removing tuple with key 5\n",
      "removing tuple with key 40\n",
      "removing tuple with key 'i\n",
      "removing tuple with key 'it's\n",
      "removing tuple with key 3\n",
      "removing tuple with key 3000\n",
      "removing tuple with key 1975\n",
      "removing tuple with key 95\n",
      "removing tuple with key 50\n",
      "removing tuple with key 86\n",
      "removing tuple with key 1960s\n",
      "removing tuple with key 84\n",
      "removing tuple with key 'what\n",
      "removing tuple with key ]\n",
      "removing tuple with key 3d\n",
      "removing tuple with key 4\n",
      "removing tuple with key 300\n",
      "removing tuple with key 2000\n",
      "removing tuple with key 'a\n",
      "removing tuple with key =\n",
      "removing tuple with key 60\n",
      "removing tuple with key É\n",
      "removing tuple with key #9\n",
      "removing tuple with key 'a'\n",
      "removing tuple with key 1960\n",
      "removing tuple with key [it]\n",
      "removing tuple with key [t]he\n",
      "removing tuple with key 'this\n",
      "removing tuple with key 'em\n",
      "removing tuple with key 1940s\n",
      "removing tuple with key $9\n",
      "removing tuple with key 48\n",
      "removing tuple with key 1952\n",
      "removing tuple with key 1999\n",
      "removing tuple with key 'n'\n",
      "removing tuple with key 1950s\n",
      "removing tuple with key '50s\n",
      "removing tuple with key 'in\n",
      "removing tuple with key 110\n",
      "removing tuple with key 3-d\n",
      "removing tuple with key 007\n",
      "removing tuple with key 'n\n",
      "removing tuple with key ½\n",
      "removing tuple with key 19\n",
      "removing tuple with key 170\n",
      "removing tuple with key 's\n",
      "removing tuple with key 451\n",
      "removing tuple with key 'best\n",
      "removing tuple with key 1998\n",
      "removing tuple with key 1979\n",
      "removing tuple with key 1958\n",
      "removing tuple with key 25\n",
      "removing tuple with key [an]\n",
      "removing tuple with key 'life\n",
      "removing tuple with key 21/2\n",
      "removing tuple with key 9/11\n",
      "removing tuple with key 94\n",
      "removing tuple with key 'are\n",
      "removing tuple with key [but]\n",
      "removing tuple with key 89\n",
      "removing tuple with key [\n",
      "removing tuple with key 163\n",
      "removing tuple with key 45\n",
      "removing tuple with key 9-11\n",
      "removing tuple with key 7\n",
      "removing tuple with key 12\n",
      "removing tuple with key 19th\n",
      "removing tuple with key 'love\n",
      "removing tuple with key 1978\n",
      "removing tuple with key [is]\n",
      "removing tuple with key 'how\n",
      "removing tuple with key 1982\n",
      "removing tuple with key [de\n",
      "removing tuple with key 'do\n",
      "removing tuple with key 'dumb\n",
      "removing tuple with key $40\n",
      "removing tuple with key '80s\n",
      "removing tuple with key 83\n",
      "removing tuple with key 'if\n",
      "removing tuple with key 000\n",
      "removing tuple with key 'they\n",
      "removing tuple with key -\n",
      "removing tuple with key [and]\n",
      "removing tuple with key 1950\n",
      "removing tuple with key 93\n",
      "removing tuple with key 77\n",
      "removing tuple with key 'why\n",
      "removing tuple with key 'who\n",
      "removing tuple with key 1915\n",
      "removing tuple with key 1984\n",
      "removing tuple with key 1934\n",
      "removing tuple with key 2'\n",
      "removing tuple with key 'we\n",
      "removing tuple with key 'my\n",
      "removing tuple with key 85\n",
      "removing tuple with key [its]\n",
      "removing tuple with key 'red\n",
      "removing tuple with key '87\n",
      "removing tuple with key 'home\n",
      "removing tuple with key 'es\n",
      "removing tuple with key 800\n",
      "removing tuple with key [time\n",
      "removing tuple with key [gai]\n",
      "removing tuple with key 52\n",
      "removing tuple with key 88\n",
      "removing tuple with key 1989\n",
      "removing tuple with key an\n",
      "removing tuple with key 127\n",
      "removing tuple with key [b]ut\n",
      "removing tuple with key 'just\n",
      "removing tuple with key $8\n",
      "removing tuple with key 1970\n",
      "removing tuple with key 1987\n",
      "removing tuple with key único\n",
      "removing tuple with key 'face\n",
      "removing tuple with key ápice\n",
      "removing tuple with key $7\n",
      "removing tuple with key 00\n",
      "removing tuple with key 117\n",
      "removing tuple with key 1962\n",
      "removing tuple with key 'i'm\n",
      "removing tuple with key 'time\n",
      "removing tuple with key [as\n",
      "removing tuple with key 'blue\n",
      "removing tuple with key 75\n",
      "removing tuple with key 'too\n",
      "removing tuple with key 'ick'\n",
      "removing tuple with key 100%\n",
      "removing tuple with key '70's\n",
      "removing tuple with key what\n",
      "removing tuple with key 104\n",
      "removing tuple with key 1967\n",
      "removing tuple with key 24/7\n",
      "removing tuple with key 'dog'\n",
      "removing tuple with key 'quit\n",
      "removing tuple with key 51st\n",
      "removing tuple with key 96\n",
      "removing tuple with key 'no\n",
      "removing tuple with key 270\n",
      "removing tuple with key às\n",
      "removing tuple with key 15th\n",
      "removing tuple with key [hell\n",
      "removing tuple with key 42\n",
      "removing tuple with key -spy\n",
      "removing tuple with key 1933\n",
      "removing tuple with key 'hip'\n",
      "removing tuple with key [n]o\n",
      "removing tuple with key 1990\n",
      "removing tuple with key 'old\n",
      "removing tuple with key [of\n",
      "removing tuple with key [i]f\n",
      "removing tuple with key [a]n\n",
      "removing tuple with key 60s\n",
      "removing tuple with key 8th\n",
      "removing tuple with key 1873\n",
      "removing tuple with key 'rare\n",
      "removing tuple with key 70s\n",
      "removing tuple with key 1994\n",
      "removing tuple with key 26\n",
      "removing tuple with key 295\n",
      "removing tuple with key 1997\n",
      "removing tuple with key 180\n",
      "removing tuple with key 8-10\n",
      "removing tuple with key 98\n",
      "removing tuple with key [on\n",
      "removing tuple with key 123\n",
      "removing tuple with key '30s\n",
      "removing tuple with key '40s\n",
      "removing tuple with key 'back\n",
      "removing tuple with key 1930s\n",
      "removing tuple with key '50's\n",
      "removing tuple with key 'til\n",
      "removing tuple with key [jack\n",
      "removing tuple with key 1993\n",
      "removing tuple with key 1991\n",
      "removing tuple with key 1899\n",
      "removing tuple with key 'chan\n",
      "removing tuple with key warm\n",
      "removing tuple with key 10th\n",
      "removing tuple with key 1/2\n",
      "removing tuple with key 'bad'\n",
      "removing tuple with key [næs]\n",
      "removing tuple with key 1938\n",
      "removing tuple with key 'de\n",
      "removing tuple with key #3\n",
      "removing tuple with key 1972\n",
      "removing tuple with key $20\n",
      "removing tuple with key 65th\n",
      "removing tuple with key 1995\n",
      "removing tuple with key 'uhhh\n",
      "removing tuple with key 1953\n",
      "removing tuple with key 11th\n",
      "removing tuple with key 2's\n",
      "removing tuple with key 'si\n",
      "removing tuple with key 'lick\n",
      "removing tuple with key élan\n",
      "removing tuple with key 'top\n",
      "removing tuple with key 72\n",
      "Len After removing special symbols 15000\n",
      "Len after removing nouns and verbs 4125\n"
     ]
    }
   ],
   "source": [
    "# Defining classifier, based on only adjectives and exclaimatory marks\n",
    "\n",
    "sentences = sentence_polarity.sents()\n",
    "print(sentence_polarity.categories())\n",
    "documents = [(sent, cat) for cat in sentence_polarity.categories() \n",
    "    for sent in sentence_polarity.sents(categories=cat)]\n",
    "\n",
    "random.shuffle(documents)\n",
    "\n",
    "# all_word_list after removing the stop words\n",
    "all_words_list = [word for (sent,cat) in documents for word in sent if word not in new_stop_words]\n",
    "\n",
    "all_words = nltk.FreqDist(all_words_list)\n",
    "word_items = all_words.most_common(15000)\n",
    "\n",
    "# we can see that all_words_list currently has all the words like nouns\n",
    "# adjectives, pronouns, exclaimation marks, proper nouns, etc\n",
    "# print(word_items)\n",
    "\n",
    "# Since we know that classification of sentiment is purely based on the adjectives\n",
    "# negative words, etc\n",
    "\n",
    "# lets remove all the special symbols from the list\n",
    "\n",
    "print(type(word_items))\n",
    "word_dict = dict(word_items)\n",
    "print(type(word_dict))\n",
    "\n",
    "# remove all non - alpha characters\n",
    "pattern = \"[^A-Za-z]+\"\n",
    "prog = re.compile(pattern)\n",
    "refined_word_dict = dict()\n",
    "for key in word_dict:\n",
    "    if (prog.match(key)):\n",
    "        # .,\"--)?(:';!-2*[a]902002 all such words are removed from the most frequent words\n",
    "        if (len(key)<=5):\n",
    "            print(\"removing tuple with key \"+ key)\n",
    "        else:\n",
    "            value = word_dict[key]\n",
    "        refined_word_dict[key] = value\n",
    "\n",
    "    else:\n",
    "        value = word_dict[key]\n",
    "        refined_word_dict[key] = value\n",
    "\n",
    "# adding 2 important words externally to the word_dict\n",
    "refined_word_dict['love'] = 100\n",
    "refined_word_dict['hate'] = 100\n",
    "        \n",
    "print(\"Len After removing special symbols \"+str(len(refined_word_dict)))\n",
    "word_dict = dict()\n",
    "\n",
    "# removing nouns, determinants from the list\n",
    "\n",
    "for key in refined_word_dict:\n",
    "    word_tokens = []\n",
    "    word_tokens.append(key)\n",
    "    tag = nltk.pos_tag(word_tokens) \n",
    "    tag = tag[0][1]\n",
    "    if(tag == 'NN' or tag == 'NNS'):\n",
    "        # print(\"removing Noun tuple with key \"+ key)\n",
    "        continue\n",
    "    #elif (tag == 'VB' or tag == 'VBD' or tag == 'VBN' or tag == 'VBP' or tag == 'VBZ' ):\n",
    "        # print(\"removing Verb tuple with key \" + key)\n",
    "    #    continue\n",
    "    else:\n",
    "        value = refined_word_dict[key]\n",
    "        word_dict[key] = value\n",
    "        # print(tag)\n",
    "        \n",
    "print(\"Len after removing nouns and verbs \" + str(len(word_dict)))\n",
    "# print(word_dict)\n",
    "import copy\n",
    "\n",
    "word_dict_copy = copy.copy(word_dict)\n",
    "\n",
    "# stemming\n",
    "ps = PorterStemmer()\n",
    "for key in word_dict_copy:\n",
    "    new_key = key\n",
    "    new_key = ps.stem(new_key)\n",
    "    new_value = word_dict_copy[key]\n",
    "    word_dict[new_key] = new_value\n",
    "    \n",
    "# converting dictionary to list again\n",
    "word_items = word_dict.items()\n",
    "word_features = [word for (word, freq) in word_items]\n",
    "\n",
    "\n",
    "\n",
    "# print(word_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['strongsubj', 'adj', False, 'neutral']\n",
      "2\n",
      "0\n",
      "0.73\n"
     ]
    }
   ],
   "source": [
    "SLpath = 'subjclueslen1-HLTEMNLP05.tff'\n",
    "SL = readSubjectivity(SLpath)\n",
    "print(SL['absolute'])\n",
    "\n",
    "# define the features, to find out the feature_set\n",
    "def SL_features(document, word_features, SL):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "        # count variables for the 4 classes of subjectivity\n",
    "        weakPos = 0\n",
    "        strongPos = 0\n",
    "        weakNeg = 0\n",
    "        strongNeg = 0\n",
    "        for word in document_words:\n",
    "            if word in SL:\n",
    "                strength, posTag, isStemmed, polarity = SL[word]\n",
    "                if strength == 'weaksubj' and polarity == 'positive':\n",
    "                    weakPos += 1\n",
    "                if strength == 'strongsubj' and polarity == 'positive':\n",
    "                    strongPos += 1\n",
    "                if strength == 'weaksubj' and polarity == 'negative':\n",
    "                    weakNeg += 1\n",
    "                if strength == 'strongsubj' and polarity == 'negative':\n",
    "                    strongNeg += 1\n",
    "                features['positivecount'] = weakPos + (2 * strongPos)\n",
    "                features['negativecount'] = weakNeg + (2 * strongNeg)      \n",
    "    return features\n",
    "\n",
    "\n",
    "#define the feature set for performinh the classification\n",
    "# word features here is the revised word features after removing the stop words\n",
    "SL_featuresets = [(SL_features(d, word_features, SL), c) for (d,c) in documents]\n",
    "\n",
    "print(SL_featuresets[0][0]['positivecount'])\n",
    "print(SL_featuresets[0][0]['negativecount'])\n",
    "\n",
    "train_set, test_set = SL_featuresets[1000:], SL_featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "reflist = []\n",
    "testlist = []\n",
    "for (features, label) in test_set:\n",
    "    reflist.append(label) \n",
    "    testlist.append(classifier.classify(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pos', 'pos', 'neg', 'pos', 'pos', 'neg', 'pos', 'pos', 'pos', 'pos', 'pos', 'neg', 'neg', 'neg', 'pos', 'pos', 'pos', 'pos', 'neg', 'neg', 'neg', 'neg', 'pos', 'neg', 'pos', 'neg', 'neg', 'neg', 'pos', 'pos']\n",
      "['pos', 'neg', 'neg', 'pos', 'pos', 'neg', 'pos', 'neg', 'neg', 'pos', 'pos', 'neg', 'pos', 'pos', 'pos', 'pos', 'pos', 'pos', 'neg', 'pos', 'pos', 'neg', 'pos', 'neg', 'pos', 'neg', 'neg', 'pos', 'neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "print(reflist[:30] )\n",
    "print(testlist[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(nltk.metrics)\n",
    "from nltk.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "reflist = []\n",
    "testlist = []\n",
    "for (features, label) in test_set:\n",
    "    reflist.append(label) \n",
    "    testlist.append(classifier.classify(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    |   n   p |\n",
      "    |   e   o |\n",
      "    |   g   s |\n",
      "----+---------+\n",
      "neg |<358>146 |\n",
      "pos | 125<371>|\n",
      "----+---------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = ConfusionMatrix(reflist, testlist)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.729"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "classifier=nltk.classify.SklearnClassifier(LinearSVC()).train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(353, 151, 119, 377)"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating true negative, false positive, false negative, true positive\n",
    "from sklearn.metrics import confusion_matrix\n",
    "tn, fp, fn, tp = confusion_matrix(reflist, testlist).ravel()\n",
    "(tn, fp, fn, tp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
