{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycorpus = PlaintextCorpusReader('.','.*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['state_union_part1.txt', 'state_union_part2.txt', 'state_union_policy.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mycorpus.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    " part2 = mycorpus.fileids()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'state_union_part2.txt'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "part2string = mycorpus.raw('state_union_part2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    " part2tokens = nltk.word_tokenize(part2string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Project',\n",
       " 'Gutenberg',\n",
       " 'EBook',\n",
       " 'of',\n",
       " 'Complete',\n",
       " 'State',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Union',\n",
       " 'Addresses',\n",
       " ',',\n",
       " 'from',\n",
       " '1946',\n",
       " 'to',\n",
       " 'the',\n",
       " 'Present',\n",
       " '(',\n",
       " '#',\n",
       " '41',\n",
       " 'in',\n",
       " 'our',\n",
       " 'series',\n",
       " 'of',\n",
       " 'US',\n",
       " 'Presidential',\n",
       " 'State',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Union',\n",
       " 'Addresses',\n",
       " ')',\n",
       " 'Copyright',\n",
       " 'laws',\n",
       " 'are',\n",
       " 'changing',\n",
       " 'all',\n",
       " 'over',\n",
       " 'the',\n",
       " 'world',\n",
       " '.',\n",
       " 'Be',\n",
       " 'sure',\n",
       " 'to',\n",
       " 'check',\n",
       " 'the',\n",
       " 'copyright',\n",
       " 'laws',\n",
       " 'for',\n",
       " 'your',\n",
       " 'country',\n",
       " 'before',\n",
       " 'downloading',\n",
       " 'or',\n",
       " 'redistributing',\n",
       " 'this',\n",
       " 'or',\n",
       " 'any',\n",
       " 'other',\n",
       " 'Project',\n",
       " 'Gutenberg',\n",
       " 'eBook',\n",
       " '.',\n",
       " 'This',\n",
       " 'header',\n",
       " 'should',\n",
       " 'be',\n",
       " 'the',\n",
       " 'first',\n",
       " 'thing',\n",
       " 'seen',\n",
       " 'when',\n",
       " 'viewing',\n",
       " 'this',\n",
       " 'Project',\n",
       " 'Gutenberg',\n",
       " 'file',\n",
       " '.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'not',\n",
       " 'remove',\n",
       " 'it',\n",
       " '.',\n",
       " 'Do',\n",
       " 'not',\n",
       " 'change',\n",
       " 'or',\n",
       " 'edit',\n",
       " 'the',\n",
       " 'header',\n",
       " 'without',\n",
       " 'written',\n",
       " 'permission',\n",
       " '.',\n",
       " 'Please',\n",
       " 'read',\n",
       " 'the',\n",
       " '``',\n",
       " 'legal']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part2tokens[:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2621720"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(part2string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "484221"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(part2tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphapart2 = [w for w in part2tokens if w.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Project',\n",
       " 'Gutenberg',\n",
       " 'EBook',\n",
       " 'of',\n",
       " 'Complete',\n",
       " 'State',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Union',\n",
       " 'Addresses',\n",
       " 'from',\n",
       " 'to',\n",
       " 'the',\n",
       " 'Present',\n",
       " 'in',\n",
       " 'our',\n",
       " 'series',\n",
       " 'of',\n",
       " 'US',\n",
       " 'Presidential',\n",
       " 'State',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Union',\n",
       " 'Addresses',\n",
       " 'Copyright',\n",
       " 'laws',\n",
       " 'are',\n",
       " 'changing',\n",
       " 'all',\n",
       " 'over',\n",
       " 'the',\n",
       " 'world',\n",
       " 'Be',\n",
       " 'sure',\n",
       " 'to',\n",
       " 'check',\n",
       " 'the',\n",
       " 'copyright',\n",
       " 'laws',\n",
       " 'for',\n",
       " 'your',\n",
       " 'country',\n",
       " 'before',\n",
       " 'downloading',\n",
       " 'or',\n",
       " 'redistributing',\n",
       " 'this',\n",
       " 'or',\n",
       " 'any',\n",
       " 'other',\n",
       " 'Project',\n",
       " 'Gutenberg',\n",
       " 'eBook',\n",
       " 'This',\n",
       " 'header',\n",
       " 'should',\n",
       " 'be',\n",
       " 'the',\n",
       " 'first',\n",
       " 'thing',\n",
       " 'seen',\n",
       " 'when',\n",
       " 'viewing',\n",
       " 'this',\n",
       " 'Project',\n",
       " 'Gutenberg',\n",
       " 'file',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'not',\n",
       " 'remove',\n",
       " 'it',\n",
       " 'Do',\n",
       " 'not',\n",
       " 'change',\n",
       " 'or',\n",
       " 'edit',\n",
       " 'the',\n",
       " 'header',\n",
       " 'without',\n",
       " 'written',\n",
       " 'permission',\n",
       " 'Please',\n",
       " 'read',\n",
       " 'the',\n",
       " 'legal',\n",
       " 'small',\n",
       " 'print',\n",
       " 'and',\n",
       " 'other',\n",
       " 'information',\n",
       " 'about',\n",
       " 'the',\n",
       " 'eBook',\n",
       " 'and',\n",
       " 'Project',\n",
       " 'Gutenberg',\n",
       " 'at']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " alphapart2[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphalowerpart2 = [w.lower( ) for w in alphapart2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'project',\n",
       " 'gutenberg',\n",
       " 'ebook',\n",
       " 'of',\n",
       " 'complete',\n",
       " 'state',\n",
       " 'of',\n",
       " 'the',\n",
       " 'union',\n",
       " 'addresses',\n",
       " 'from',\n",
       " 'to',\n",
       " 'the',\n",
       " 'present',\n",
       " 'in',\n",
       " 'our',\n",
       " 'series',\n",
       " 'of',\n",
       " 'us',\n",
       " 'presidential',\n",
       " 'state',\n",
       " 'of',\n",
       " 'the',\n",
       " 'union',\n",
       " 'addresses',\n",
       " 'copyright',\n",
       " 'laws',\n",
       " 'are',\n",
       " 'changing',\n",
       " 'all',\n",
       " 'over',\n",
       " 'the',\n",
       " 'world',\n",
       " 'be',\n",
       " 'sure',\n",
       " 'to',\n",
       " 'check',\n",
       " 'the',\n",
       " 'copyright',\n",
       " 'laws',\n",
       " 'for',\n",
       " 'your',\n",
       " 'country',\n",
       " 'before',\n",
       " 'downloading',\n",
       " 'or',\n",
       " 'redistributing',\n",
       " 'this',\n",
       " 'or']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphalowerpart2[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "len(stopwords)\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoppedalphalowerpart2 = [w for w in alphalowerpart2 if w not in stopwords]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(stoppedalphalowerpart2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdistkeys = list(fdist.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['project',\n",
       " 'gutenberg',\n",
       " 'ebook',\n",
       " 'complete',\n",
       " 'state',\n",
       " 'union',\n",
       " 'addresses',\n",
       " 'present',\n",
       " 'series',\n",
       " 'us',\n",
       " 'presidential',\n",
       " 'copyright',\n",
       " 'laws',\n",
       " 'changing',\n",
       " 'world',\n",
       " 'sure',\n",
       " 'check',\n",
       " 'country',\n",
       " 'downloading',\n",
       " 'redistributing',\n",
       " 'header',\n",
       " 'first',\n",
       " 'thing',\n",
       " 'seen',\n",
       " 'viewing',\n",
       " 'file',\n",
       " 'please',\n",
       " 'remove',\n",
       " 'change',\n",
       " 'edit',\n",
       " 'without',\n",
       " 'written',\n",
       " 'permission',\n",
       " 'read',\n",
       " 'legal',\n",
       " 'small',\n",
       " 'print',\n",
       " 'information',\n",
       " 'bottom',\n",
       " 'included',\n",
       " 'important',\n",
       " 'specific',\n",
       " 'rights',\n",
       " 'restrictions',\n",
       " 'may',\n",
       " 'used',\n",
       " 'also',\n",
       " 'find',\n",
       " 'make',\n",
       " 'donation']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdistkeys[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "topkeys = fdist.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('must', 1628)\n",
      "('people', 1506)\n",
      "('world', 1490)\n",
      "('new', 1441)\n",
      "('america', 1271)\n",
      "('year', 1265)\n",
      "('congress', 1230)\n",
      "('us', 1216)\n",
      "('government', 1111)\n",
      "('years', 1111)\n",
      "('american', 950)\n",
      "('nation', 861)\n",
      "('one', 804)\n",
      "('every', 780)\n",
      "('make', 778)\n",
      "('work', 754)\n",
      "('federal', 744)\n",
      "('time', 741)\n",
      "('states', 711)\n",
      "('americans', 688)\n",
      "('help', 686)\n",
      "('security', 685)\n",
      "('war', 674)\n",
      "('economic', 671)\n",
      "('peace', 668)\n",
      "('united', 651)\n",
      "('nations', 645)\n",
      "('also', 639)\n",
      "('program', 638)\n",
      "('country', 630)\n",
      "('national', 609)\n",
      "('economy', 588)\n",
      "('great', 583)\n",
      "('last', 572)\n",
      "('many', 563)\n",
      "('free', 558)\n",
      "('need', 554)\n",
      "('first', 553)\n",
      "('let', 549)\n",
      "('would', 548)\n",
      "('state', 520)\n",
      "('tax', 514)\n",
      "('know', 507)\n",
      "('million', 507)\n",
      "('freedom', 503)\n",
      "('budget', 501)\n",
      "('health', 489)\n",
      "('future', 475)\n",
      "('system', 463)\n",
      "('programs', 462)\n"
     ]
    }
   ],
   "source": [
    "for pair in topkeys:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram Freq Dist\n",
    "Another way to look for interesting characterizations of a corpus is to look at pairs of words that are frequently collocated, that is, they occur in a sequence called a bigram.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We start by making an object called a BigramCollocationFinder.  The finder then allows us to call other functions to filter the bigrams that it collected and to give scores to the bigrams.  As a first step, we can score the bigrams by frequency.  Be sure to use the original tokens from the document and not the filtered tokens that you might create to look at word frequencies. For example, the bigrams of the sentence “run after the fox!”  include “run after”, “after the”, and “the fox”. If you use the original tokens you will obtain these bigrams. If you filter the sentence with the stop words list, then you may not have “the” after the filtering. Then your bigrams will have “after fox” – which is not a bigram in the original dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder = BigramCollocationFinder.from_words(alphalowerpart2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('of', 'the'), 0.007686567516636518)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(scored)\n",
    "first = scored[0]\n",
    "type(first)\n",
    "first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('of', 'the'), 0.007686567516636518)\n",
      "(('in', 'the'), 0.005889492899075722)\n",
      "(('to', 'the'), 0.0033414725149126493)\n",
      "(('of', 'our'), 0.003183254381697704)\n",
      "(('and', 'the'), 0.002555104778187623)\n",
      "(('for', 'the'), 0.0024063325036720777)\n",
      "(('we', 'have'), 0.00233076563407688)\n",
      "(('we', 'must'), 0.0021819933595613344)\n",
      "(('the', 'world'), 0.0019718230034996907)\n",
      "(('the', 'congress'), 0.0016813628484931495)\n",
      "(('will', 'be'), 0.001492445674505155)\n",
      "(('we', 'are'), 0.0014664695630818058)\n",
      "(('it', 'is'), 0.0014475778456830065)\n",
      "(('we', 'can'), 0.0014404934516584567)\n",
      "(('on', 'the'), 0.0014381319869836067)\n",
      "(('the', 'united'), 0.0013602036527135591)\n",
      "(('we', 'will'), 0.0013365890059650598)\n",
      "(('with', 'the'), 0.00132714314726566)\n",
      "(('and', 'to'), 0.001291721177142911)\n",
      "(('in', 'our'), 0.0012775523890938116)\n",
      "(('that', 'the'), 0.0012421304189710626)\n",
      "(('by', 'the'), 0.0012137928428728635)\n",
      "(('that', 'we'), 0.0011453103673022156)\n",
      "(('united', 'states'), 0.0010909966797806672)\n",
      "(('and', 'we'), 0.0010744664270567177)\n",
      "(('in', 'this'), 0.0010697434977070177)\n",
      "(('to', 'be'), 0.0010555747096579182)\n",
      "(('more', 'than'), 0.0009894536987621202)\n",
      "(('in', 'a'), 0.0009729234460381707)\n",
      "(('of', 'a'), 0.0009682005166884709)\n",
      "(('and', 'i'), 0.000965839052013621)\n",
      "(('has', 'been'), 0.0009587546579890711)\n",
      "(('is', 'the'), 0.0009138868291669224)\n",
      "(('have', 'been'), 0.0008997180411178229)\n",
      "(('of', 'this'), 0.0008855492530687233)\n",
      "(('the', 'american'), 0.000866657535669924)\n",
      "(('a', 'new'), 0.0008406814242465747)\n",
      "(('i', 'have'), 0.0008005365247741259)\n",
      "(('is', 'a'), 0.0007745604133507767)\n",
      "(('must', 'be'), 0.0007745604133507767)\n",
      "(('to', 'make'), 0.0007698374840010768)\n",
      "(('from', 'the'), 0.000762753089976527)\n",
      "(('as', 'a'), 0.0007462228372525775)\n",
      "(('to', 'help'), 0.0007462228372525775)\n",
      "(('state', 'of'), 0.0007414999079028777)\n",
      "(('at', 'the'), 0.0007202467258292283)\n",
      "(('the', 'people'), 0.0007155237964795285)\n",
      "(('can', 'not'), 0.0006801018263567796)\n",
      "(('if', 'we'), 0.0006801018263567796)\n",
      "(('to', 'our'), 0.0006801018263567796)\n"
     ]
    }
   ],
   "source": [
    "for bscore in scored[:50]:\n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying filters now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before that, adding the 'united', 'states' in the list of stop words, because these words will definitely occur frequently in this document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.append(\"united\")\n",
    "stopwords.append(\"states\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('american', 'people'), 0.000564390057289133)\n",
      "(('last', 'year'), 0.000531329551841234)\n",
      "(('fiscal', 'year'), 0.0004392324295220868)\n",
      "(('federal', 'government'), 0.00043450950017238693)\n",
      "(('social', 'security'), 0.00042742510614783715)\n",
      "(('health', 'care'), 0.00042034071212328737)\n",
      "(('let', 'us'), 0.00041089485342388763)\n",
      "(('years', 'ago'), 0.00038255727732568846)\n",
      "(('union', 'address'), 0.00032588212512929017)\n",
      "(('billion', 'dollars'), 0.00030699040773049074)\n",
      "(('million', 'dollars'), 0.00029990601370594096)\n",
      "(('soviet', 'union'), 0.0002951830843562411)\n",
      "(('free', 'world'), 0.0002550381848837923)\n",
      "(('every', 'american'), 0.00023378500281014296)\n",
      "(('economic', 'growth'), 0.0002219776794358933)\n",
      "(('middle', 'east'), 0.00021489328541134353)\n",
      "(('make', 'sure'), 0.00020780889138679375)\n",
      "(('free', 'nations'), 0.00019836303268739404)\n",
      "(('first', 'time'), 0.00019127863866284423)\n",
      "(('four', 'years'), 0.00019127863866284423)\n",
      "(('armed', 'forces'), 0.00017710985061374467)\n",
      "(('must', 'continue'), 0.00017474838593889474)\n",
      "(('world', 'war'), 0.00017474838593889474)\n",
      "(('work', 'together'), 0.0001700254565891949)\n",
      "(('foreign', 'policy'), 0.00016530252723949502)\n",
      "(('new', 'jobs'), 0.00016530252723949502)\n",
      "(('two', 'years'), 0.00015821813321494524)\n",
      "(('vice', 'president'), 0.00015821813321494524)\n",
      "(('next', 'years'), 0.0001558566685400953)\n",
      "(('national', 'security'), 0.0001464108098406956)\n",
      "(('must', 'also'), 0.00014404934516584566)\n",
      "(('address', 'january'), 0.00014168788049099575)\n",
      "(('human', 'rights'), 0.0001393264158161458)\n",
      "(('health', 'insurance'), 0.00013696495114129588)\n",
      "(('fellow', 'americans'), 0.00013224202179159603)\n",
      "(('fellow', 'citizens'), 0.00013224202179159603)\n",
      "(('past', 'year'), 0.00013224202179159603)\n",
      "(('civil', 'rights'), 0.00012751909244189616)\n",
      "(('young', 'people'), 0.00012751909244189616)\n",
      "(('past', 'years'), 0.0001227961630921963)\n",
      "(('private', 'sector'), 0.0001227961630921963)\n",
      "(('god', 'bless'), 0.00012043469841734637)\n",
      "(('local', 'governments'), 0.00012043469841734637)\n",
      "(('nuclear', 'weapons'), 0.00012043469841734637)\n",
      "(('interest', 'rates'), 0.00011571176906764651)\n",
      "(('next', 'year'), 0.00011571176906764651)\n",
      "(('balanced', 'budget'), 0.00011335030439279659)\n",
      "(('must', 'make'), 0.00011335030439279659)\n",
      "(('high', 'school'), 0.00011098883971794665)\n",
      "(('minimum', 'wage'), 0.00011098883971794665)\n"
     ]
    }
   ],
   "source": [
    "finder.apply_word_filter(lambda w: w in stopwords)\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "for bscore in scored[:50]:\n",
    "    print (bscore)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### def apply_ngram_filter(self, fn):\n",
    "        # \"Removes candidate ngrams (w1, w2, ...) where fn(w1, w2, ...) evaluates to True.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('american', 'people'), 0.000564390057289133)\n",
      "(('last', 'year'), 0.000531329551841234)\n",
      "(('fiscal', 'year'), 0.0004392324295220868)\n",
      "(('federal', 'government'), 0.00043450950017238693)\n",
      "(('social', 'security'), 0.00042742510614783715)\n",
      "(('health', 'care'), 0.00042034071212328737)\n",
      "(('years', 'ago'), 0.00038255727732568846)\n",
      "(('union', 'address'), 0.00032588212512929017)\n",
      "(('billion', 'dollars'), 0.00030699040773049074)\n",
      "(('million', 'dollars'), 0.00029990601370594096)\n",
      "(('soviet', 'union'), 0.0002951830843562411)\n",
      "(('free', 'world'), 0.0002550381848837923)\n",
      "(('every', 'american'), 0.00023378500281014296)\n",
      "(('economic', 'growth'), 0.0002219776794358933)\n",
      "(('middle', 'east'), 0.00021489328541134353)\n",
      "(('make', 'sure'), 0.00020780889138679375)\n",
      "(('free', 'nations'), 0.00019836303268739404)\n",
      "(('first', 'time'), 0.00019127863866284423)\n",
      "(('four', 'years'), 0.00019127863866284423)\n",
      "(('armed', 'forces'), 0.00017710985061374467)\n",
      "(('must', 'continue'), 0.00017474838593889474)\n",
      "(('world', 'war'), 0.00017474838593889474)\n",
      "(('work', 'together'), 0.0001700254565891949)\n",
      "(('foreign', 'policy'), 0.00016530252723949502)\n",
      "(('vice', 'president'), 0.00015821813321494524)\n",
      "(('next', 'years'), 0.0001558566685400953)\n",
      "(('national', 'security'), 0.0001464108098406956)\n",
      "(('must', 'also'), 0.00014404934516584566)\n",
      "(('address', 'january'), 0.00014168788049099575)\n",
      "(('human', 'rights'), 0.0001393264158161458)\n",
      "(('health', 'insurance'), 0.00013696495114129588)\n",
      "(('fellow', 'americans'), 0.00013224202179159603)\n",
      "(('fellow', 'citizens'), 0.00013224202179159603)\n",
      "(('past', 'year'), 0.00013224202179159603)\n",
      "(('civil', 'rights'), 0.00012751909244189616)\n",
      "(('young', 'people'), 0.00012751909244189616)\n",
      "(('past', 'years'), 0.0001227961630921963)\n",
      "(('private', 'sector'), 0.0001227961630921963)\n",
      "(('local', 'governments'), 0.00012043469841734637)\n",
      "(('nuclear', 'weapons'), 0.00012043469841734637)\n",
      "(('interest', 'rates'), 0.00011571176906764651)\n",
      "(('next', 'year'), 0.00011571176906764651)\n",
      "(('balanced', 'budget'), 0.00011335030439279659)\n",
      "(('must', 'make'), 0.00011335030439279659)\n",
      "(('high', 'school'), 0.00011098883971794665)\n",
      "(('minimum', 'wage'), 0.00011098883971794665)\n",
      "(('white', 'house'), 0.00010862737504309673)\n",
      "(('cold', 'war'), 0.00010154298101854694)\n",
      "(('middle', 'class'), 0.00010154298101854694)\n",
      "(('last', 'years'), 9.918151634369702e-05)\n"
     ]
    }
   ],
   "source": [
    "finder.apply_ngram_filter(lambda w1, w2: len(w1) < 4)\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "for bscore in scored[:50]:\n",
    "    print (bscore)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information and other scores\n",
    "Recall that Mutual Information is a score introduced in the paper by Church and Hanks, where they defined it as an Association Ratio. Note that technically the original information theoretic definition of mutual information allows the two words to be in either order, but that the association ratio defined by Church and Hanks requires the words to be in order from left to right wherever they appear in the window\n",
    "\n",
    "In NLTK, the mutual information score is given by a function for Pointwise Mutual Information, where this is the version without the window.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder2 = BigramCollocationFinder.from_words(stoppedalphalowerpart2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder2.apply_freq_filter(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('abating', 'hurried'), 17.74922784260267)\n",
      "(('adnan', 'pachachi'), 17.74922784260267)\n",
      "(('airman', 'coastguardsman'), 17.74922784260267)\n",
      "(('akin', 'treason'), 17.74922784260267)\n",
      "(('alianza', 'para'), 17.74922784260267)\n",
      "(('allure', 'permissive'), 17.74922784260267)\n",
      "(('ancestry', 'gender'), 17.74922784260267)\n",
      "(('annetter', 'strauss'), 17.74922784260267)\n",
      "(('anniversaries', 'stolen'), 17.74922784260267)\n",
      "(('appraisers', 'designated'), 17.74922784260267)\n",
      "(('arousing', 'hatreds'), 17.74922784260267)\n",
      "(('arresting', 'deterioration'), 17.74922784260267)\n",
      "(('arthur', 'vandenburg'), 17.74922784260267)\n",
      "(('attendants', 'hermis'), 17.74922784260267)\n",
      "(('awarding', 'nobel'), 17.74922784260267)\n",
      "(('bali', 'jakarta'), 17.74922784260267)\n",
      "(('bans', 'insider'), 17.74922784260267)\n",
      "(('barbaric', 'captivity'), 17.74922784260267)\n",
      "(('barbed', 'wire'), 17.74922784260267)\n",
      "(('baton', 'rouge'), 17.74922784260267)\n",
      "(('belarus', 'kazakhstan'), 17.74922784260267)\n",
      "(('belgian', 'endive'), 17.74922784260267)\n",
      "(('belligerent', 'unrepentant'), 17.74922784260267)\n",
      "(('bendjedid', 'kings'), 17.74922784260267)\n",
      "(('bernard', 'baruch'), 17.74922784260267)\n",
      "(('bewilder', 'confuse'), 17.74922784260267)\n",
      "(('bisexual', 'transgender'), 17.74922784260267)\n",
      "(('blamed', 'mountainous'), 17.74922784260267)\n",
      "(('blindfolded', 'hobbled'), 17.74922784260267)\n",
      "(('blockbusters', 'belly'), 17.74922784260267)\n",
      "(('blot', 'curse'), 17.74922784260267)\n",
      "(('bombardment', 'quemoy'), 17.74922784260267)\n",
      "(('booms', 'headlong'), 17.74922784260267)\n",
      "(('boon', 'bane'), 17.74922784260267)\n",
      "(('boondoggles', 'pork'), 17.74922784260267)\n",
      "(('bore', 'brunt'), 17.74922784260267)\n",
      "(('bruce', 'randolph'), 17.74922784260267)\n",
      "(('buckingham', 'palace'), 17.74922784260267)\n",
      "(('cabot', 'lodge'), 17.74922784260267)\n",
      "(('canadians', 'italians'), 17.74922784260267)\n",
      "(('cannon', 'fodder'), 17.74922784260267)\n",
      "(('capps', 'sonny'), 17.74922784260267)\n",
      "(('carpenters', 'plumbers'), 17.74922784260267)\n",
      "(('carrybacks', 'recomputed'), 17.74922784260267)\n",
      "(('carve', 'boondoggles'), 17.74922784260267)\n",
      "(('catholic', 'protestant'), 17.74922784260267)\n",
      "(('cattle', 'goats'), 17.74922784260267)\n",
      "(('celebrates', 'lapel'), 17.74922784260267)\n",
      "(('cerebral', 'aneurysm'), 17.74922784260267)\n",
      "(('cfr', 'p'), 17.74922784260267)\n"
     ]
    }
   ],
   "source": [
    "scored = finder2.score_ngrams(bigram_measures.pmi)\n",
    "for bscore in scored[:50]:\n",
    "    print (bscore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
